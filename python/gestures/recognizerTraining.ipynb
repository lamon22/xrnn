{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: skl2onnx in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (1.18.0)\n",
      "Requirement already satisfied: tensorflow in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (2.19.0)\n",
      "Requirement already satisfied: tf2onnx in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: onnx>=1.2.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from skl2onnx) (1.17.0)\n",
      "Requirement already satisfied: onnxconverter-common>=1.7.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from skl2onnx) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/michelelamon/Documents/UNITN/AdvancedHCI/.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas scikit-learn skl2onnx tensorflow tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata\u001b[m\u001b[m                     \u001b[34moutput\u001b[m\u001b[m                   recognizerServer.ipynb\n",
      "info.txt                 \u001b[34mpositions\u001b[m\u001b[m                recognizerTraining.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 60hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with CSV files – each file contains one gesture of 30 frames\n",
    "data_folder = os.path.join(os.getcwd(), 'newData')\n",
    "csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "new_model = os.path.join(os.getcwd(), 'newModel')\n",
    "\n",
    "flattened_samples = []\n",
    "labels = []\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    # Optionally, sort by frame if necessary:\n",
    "    df = df.sort_values(by='Frame')\n",
    "    # Map hand strings to numbers if needed:\n",
    "    df['Hand'] = df['Hand'].map({'left': 1, 'right': 0})\n",
    "    # Confirm that all rows in file belong to same gesture:\n",
    "    gesture_label = df['Label'].iloc[0]\n",
    "    # Remove non-feature columns if desired (keep if they are features too)\n",
    "    # Here we remove 'Frame' and 'Label'\n",
    "    feature_df = df.drop(columns=['Frame', 'Label'])\n",
    "    # Flatten the 2D array (shape 30 x n_features) into 1D vector\n",
    "    flattened = feature_df.to_numpy().flatten()  \n",
    "    flattened_samples.append(flattened)\n",
    "    labels.append(gesture_label)\n",
    "\n",
    "# Create a new DataFrame – columns can be auto-named or you can specify them explicitly.\n",
    "# For example, the feature dimension is 30 * (number of feature columns in one file)\n",
    "num_features = flattened_samples[0].shape[0]\n",
    "columns = [f\"f{i}\" for i in range(num_features)]\n",
    "new_df = pd.DataFrame(flattened_samples, columns=columns)\n",
    "new_df['Label'] = labels\n",
    "\n",
    "# Optionally, save the reshaped data to a new CSV file\n",
    "# output_csv = os.path.join(data_folder, \"reshaped_gesture_data_30.csv\")\n",
    "# new_df.to_csv(output_csv, index=False)\n",
    "# print(f\"Reshaped data saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with CSV files – each file contains one gesture of 30 frames (60Hz recordings)\n",
    "data_folder = os.path.join(os.getcwd(), 'newData')\n",
    "csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "new_model = os.path.join(os.getcwd(), 'newModel')\n",
    "\n",
    "samples_even = []\n",
    "samples_odd = []\n",
    "labels = []\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    # Optionally, sort by frame if necessary:\n",
    "    df = df.sort_values(by='Frame')\n",
    "    # Map hand strings to numbers if needed:\n",
    "    df['Hand'] = df['Hand'].map({'left': 1, 'right': 0})\n",
    "    \n",
    "    # Confirm that all rows in file belong to the same gesture and get its label\n",
    "    gesture_label = df['Label'].iloc[0]\n",
    "    # Remove non-feature columns ('Frame' and 'Label'); keep if you want to use 'Hand' as a feature\n",
    "    feature_df = df.drop(columns=['Frame', 'Label'])\n",
    "    \n",
    "    # Split into even and odd frames:\n",
    "    even_df = feature_df.iloc[::2]  # frames 0,2,4,...,28 (15 frames)\n",
    "    odd_df  = feature_df.iloc[1::2]  # frames 1,3,5,...,29 (15 frames)\n",
    "    \n",
    "    # Flatten each subset into a 1D vector\n",
    "    flattened_even = even_df.to_numpy().flatten()\n",
    "    flattened_odd  = odd_df.to_numpy().flatten()\n",
    "    \n",
    "    samples_even.append(flattened_even)\n",
    "    samples_odd.append(flattened_odd)\n",
    "    labels.append(gesture_label)  # same label for both splits\n",
    "\n",
    "# Create DataFrames for even and odd samples. They should have the same number of features.\n",
    "num_features = samples_even[0].shape[0]\n",
    "columns = [f\"f{i}\" for i in range(num_features)]\n",
    "\n",
    "df_even = pd.DataFrame(samples_even, columns=columns)\n",
    "df_even['Label'] = labels\n",
    "#df_even['SampleType'] = 'even'   # Optional field to know sample origin\n",
    "\n",
    "df_odd = pd.DataFrame(samples_odd, columns=columns)\n",
    "df_odd['Label'] = labels\n",
    "#df_odd['SampleType'] = 'odd'\n",
    "\n",
    "# Combine both to get a 15Hz dataset with doubled samples\n",
    "df_15hz = pd.concat([df_even, df_odd], ignore_index=True)\n",
    "\n",
    "# Optionally, save the new DataFrame to a CSV file\n",
    "# output_csv = os.path.join(data_folder, \"reshaped_gesture_data_15hz.csv\")\n",
    "# df_15hz.to_csv(output_csv, index=False)\n",
    "# print(f\"Saved 15Hz dataset with even/odd split to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation scores on training set: [0.94339623 0.94339623 0.8490566  0.83018868 0.80769231]\n",
      "Mean CV score: 0.8747460087082729\n",
      "Validation Accuracy: 0.8863636363636364\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     destroy       0.89      1.00      0.94        16\n",
      "        idle       0.88      0.88      0.88        40\n",
      "        pick       0.85      0.69      0.76        16\n",
      "       prune       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.89        88\n",
      "   macro avg       0.89      0.89      0.89        88\n",
      "weighted avg       0.88      0.89      0.88        88\n",
      "\n",
      "Test Accuracy: 0.9318181818181818\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     destroy       0.94      1.00      0.97        16\n",
      "        idle       0.90      0.95      0.93        40\n",
      "        pick       1.00      0.75      0.86        16\n",
      "       prune       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.93        88\n",
      "   macro avg       0.95      0.93      0.93        88\n",
      "weighted avg       0.94      0.93      0.93        88\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/michelelamon/Documents/UNITN/AdvancedHCI/xrnn/python/gestures/newModel/scaler.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_df = new_df\n",
    "data_df = df_15hz\n",
    "\n",
    "# Define feature columns and target. Here we use all columns except 'Label' as features.\n",
    "feature_cols = [col for col in data_df.columns if col not in ['Label']]\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df['Label'].values\n",
    "\n",
    "# Split data: 60% training, 20% validation, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "# Note: 0.25 * 0.8 = 0.2, so overall 60% train, 20% validation, 20% test\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the SVM model\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training set (using StratifiedKFold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "cv_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=cv)\n",
    "print(\"5-Fold Cross-Validation scores on training set:\", cv_scores)\n",
    "print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the SVM classifier using the full training set\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = svm_model.predict(X_val_scaled)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Test the model\n",
    "y_test_pred = svm_model.predict(X_test_scaled)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save the model and scaler for later use\n",
    "joblib.dump(svm_model, os.path.join(new_model, \"svm_model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(new_model, \"scaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Directory with your CSV files\n",
    "# data_folder = os.path.join(os.getcwd(), 'data')\n",
    "# csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "\n",
    "# # Read and combine all CSV files\n",
    "# df_list = [pd.read_csv(f) for f in csv_files]\n",
    "# data_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# # Map hand strings to numbers (if you want to retain it)\n",
    "# data_df['Hand'] = data_df['Hand'].map({'left': 1, 'right': 0})\n",
    "\n",
    "# # We'll use all columns EXCEPT these non-feature ones\n",
    "# non_feature = ['Label']\n",
    "# feature_cols = [col for col in data_df.columns if col not in non_feature]\n",
    "# X = data_df[feature_cols].values\n",
    "# y = data_df['Label'].values\n",
    "\n",
    "# # Encode labels to integers and count classes\n",
    "# le = LabelEncoder()\n",
    "# y_encoded = le.fit_transform(y)\n",
    "# num_classes = len(le.classes_)\n",
    "\n",
    "# # Split data\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "# # Overall: 60% train, 20% validation, 20% test\n",
    "\n",
    "# # Normalize features (fit only on training data)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Our features count is 81 (84 columns - Frame, Hand, Label)\n",
    "# num_features = X_train.shape[1]  # should be 81\n",
    "\n",
    "# # Reshape for Conv1D: (samples, steps, channels)\n",
    "# X_train = X_train.reshape(-1, num_features, 1)\n",
    "# X_val = X_val.reshape(-1, num_features, 1)\n",
    "# X_test = X_test.reshape(-1, num_features, 1)\n",
    "\n",
    "# # Build a small 1D CNN model\n",
    "# model = Sequential([\n",
    "#     Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "#     # Second convolution to learn higher-level combinations; depth=2\n",
    "#     Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "#     GlobalAveragePooling1D(),\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Early stopping to help avoid overfitting\n",
    "# es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=16,\n",
    "#                     validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# # Save the trained model if needed\n",
    "# model.save(os.path.join(data_folder, \"small_cnn_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import tf2onnx\n",
    "\n",
    "# # Define paths\n",
    "# data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "# h5_model_path = os.path.join(data_folder, \"small_cnn_model.h5\")\n",
    "# onnx_model_path = os.path.join(data_folder, \"small_cnn_model.onnx\")\n",
    "\n",
    "# # Load the trained Keras model\n",
    "# model = tf.keras.models.load_model(h5_model_path)\n",
    "\n",
    "# # Manually set output_names if not present\n",
    "# if not hasattr(model, \"output_names\"):\n",
    "#     model.output_names = [output.name for output in model.outputs]\n",
    "\n",
    "# # Define the input signature based on your model's input shape: (None, num_features, 1)\n",
    "# input_spec = (tf.TensorSpec((None, model.input_shape[1], model.input_shape[2]), tf.float32, name=\"input\"),)\n",
    "\n",
    "# # Convert the model to ONNX format using tf2onnx\n",
    "# model_proto, external_tensor_storage = tf2onnx.convert.from_keras(\n",
    "#     model,\n",
    "#     input_signature=input_spec,\n",
    "#     opset=13,\n",
    "#     output_path=onnx_model_path\n",
    "# )\n",
    "\n",
    "# print(\"ONNX model exported to:\", onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import tf2onnx\n",
    "\n",
    "# # ------------------------\n",
    "# # Data Preparation Section\n",
    "# # ------------------------\n",
    "# data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "# csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "\n",
    "# # Read and combine CSV files\n",
    "# df_list = [pd.read_csv(f) for f in csv_files]\n",
    "# data_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# # Map hand strings to numbers (if needed)\n",
    "# data_df['Hand'] = data_df['Hand'].map({'left': 1, 'right': 0})\n",
    "\n",
    "# # Use all columns except non-feature ones: Frame, Hand, and Label\n",
    "# non_feature = ['Label']\n",
    "# feature_cols = [col for col in data_df.columns if col not in non_feature]\n",
    "# X = data_df[feature_cols].values\n",
    "# y = data_df['Label'].values\n",
    "\n",
    "# # Encode labels to integers\n",
    "# le = LabelEncoder()\n",
    "# y_encoded = le.fit_transform(y)\n",
    "# num_classes = len(le.classes_)\n",
    "\n",
    "# # Split data: 60% train, 20% validation, 20% test\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#     X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "\n",
    "# # Normalize features on training set\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Determine number of features (for example, 81 for 84 columns minus 3 non-features)\n",
    "# num_features = X_train.shape[1]\n",
    "\n",
    "# # Reshape data for Conv1D: (samples, steps, channels)\n",
    "# X_train = X_train.reshape(-1, num_features, 1)\n",
    "# X_val = X_val.reshape(-1, num_features, 1)\n",
    "# X_test = X_test.reshape(-1, num_features, 1)\n",
    "\n",
    "# # ------------------------------------\n",
    "# # Build and Train a Smaller CNN Model\n",
    "# # ------------------------------------\n",
    "# model = Sequential([\n",
    "#     # Single Conv1D layer with fewer filters\n",
    "#     Conv1D(filters=8, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "#     # Global pooling reduces parameters while still summarizing features\n",
    "#     GlobalAveragePooling1D(),\n",
    "#     # A small dense layer\n",
    "#     Dense(8, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Early stopping to help avoid overfitting\n",
    "# es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=16,\n",
    "#                     validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# # Save the Keras model to H5 format\n",
    "# h5_model_path = os.path.join(data_folder, \"small_cnn_model.h5\")\n",
    "# model.save(h5_model_path)\n",
    "# print(\"Keras model saved to:\", h5_model_path)\n",
    "\n",
    "# # ------------------------------\n",
    "# # Export the Model to ONNX Format\n",
    "# # ------------------------------\n",
    "# onnx_model_path = os.path.join(data_folder, \"small_cnn_model.onnx\")\n",
    "\n",
    "# # Load the saved Keras model (for conversion clarity)\n",
    "# model = tf.keras.models.load_model(h5_model_path)\n",
    "\n",
    "# # Manually set output names if missing\n",
    "# if not hasattr(model, \"output_names\"):\n",
    "#     model.output_names = [output.name for output in model.outputs]\n",
    "\n",
    "# # Define the input signature based on the model's input shape: (None, num_features, 1)\n",
    "# input_spec = (tf.TensorSpec((None, model.input_shape[1], model.input_shape[2]), tf.float32, name=\"input\"),)\n",
    "\n",
    "# # Convert the Keras model to ONNX using tf2onnx\n",
    "# model_proto, external_tensor_storage = tf2onnx.convert.from_keras(\n",
    "#     model,\n",
    "#     input_signature=input_spec,\n",
    "#     opset=13,\n",
    "#     output_path=onnx_model_path\n",
    "# )\n",
    "\n",
    "# print(\"ONNX model exported to:\", onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
